{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: memorize the $Q$ function qua [Model-Free Episodic Control](https://arxiv.org/abs/1606.04460)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMT API\n",
    "\n",
    "From the paper we have:\n",
    "\n",
    "1. $(u, z) \\leftarrow \\text{Query}(x)$ where $z = \\{ (x_n, \\omega_n) \\}$ is an ordered set of retrieved key-value pairs.\n",
    "1. $\\text{Update}(x, (x_n, \\omega_n), r, u)$ provides feedback reward $r$ for retrieval of $(x_n, \\omega_n)$ for query $x$.\n",
    "   1. Must be compatible with self-consistency or supervised and unsupervised updates conflict.\n",
    "1. $\\text{Insert}(x, \\omega)$ creates a new memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0,
     1,
     2,
     11,
     25,
     38,
     48,
     56,
     110,
     120,
     129,
     143
    ]
   },
   "outputs": [],
   "source": [
    "class CMT:\n",
    "    class Node:\n",
    "        def __init__(self, parent, left=None, right=None, g=None):\n",
    "            self.parent = parent\n",
    "            self.isLeaf = left is None\n",
    "            self.n = 0        \n",
    "            self.memories = {}\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.g = g\n",
    "            \n",
    "        def makeInternal(self, g):       \n",
    "            assert self.isLeaf\n",
    "            \n",
    "            self.isLeaf = False\n",
    "            self.left = CMT.Node(parent=self)\n",
    "            self.right = CMT.Node(parent=self)\n",
    "            self.n = 0\n",
    "            self.g = g\n",
    "            \n",
    "            mem = self.memories\n",
    "            self.memories = {}\n",
    "            \n",
    "            return mem\n",
    "        \n",
    "        def replaceNode(self, replacement):\n",
    "            if self is not replacement:\n",
    "                self.isLeaf = replacement.isLeaf\n",
    "                self.n = replacement.n\n",
    "                self.memories = replacement.memories\n",
    "                self.left = replacement.left\n",
    "                if self.left:\n",
    "                    self.left.parent = self\n",
    "                self.right = replacement.right\n",
    "                if self.right:\n",
    "                    self.right.parent = self\n",
    "                self.g = replacement.g\n",
    "  \n",
    "        def topk(self, x, k, f):\n",
    "            assert self.isLeaf\n",
    "            return [ z for _, z in zip(range(k), \n",
    "                                       sorted(self.memories.items(),\n",
    "                                              key=lambda z: f.predict(x, z),\n",
    "                                              reverse=True\n",
    "                                             )\n",
    "                                      ) \n",
    "                   ]\n",
    "        \n",
    "        def randk(self, k, randomState):\n",
    "            assert self.isLeaf\n",
    "            return [ z[1] for _, z in zip(range(k),\n",
    "                                          sorted( (randomState.uniform(0, 1), m) for m in self.memories.items() \n",
    "                                                )\n",
    "                                      )\n",
    "                   ]\n",
    "    \n",
    "    class Path:\n",
    "        def __init__(self, nodes, leaf):\n",
    "            self.nodes = nodes\n",
    "            self.leaf = leaf\n",
    "    \n",
    "    class LRU:\n",
    "        def __init__(self):\n",
    "            self.entries = []\n",
    "            self.entry_finder = set()\n",
    "            self.n = 0\n",
    "        \n",
    "        def add(self, x):\n",
    "            from heapq import heappush\n",
    "            \n",
    "            assert x not in self.entry_finder\n",
    "            \n",
    "            entry = (self.n, x)\n",
    "            self.entry_finder.add(x)\n",
    "            heappush(self.entries, entry)\n",
    "            self.n += 1\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.entry_finder)\n",
    "        \n",
    "        def __contains__(self, x):\n",
    "            return x in self.entry_finder\n",
    "        \n",
    "        def peek(self):\n",
    "            from heapq import heappop\n",
    "            \n",
    "            while self.entries[0][1] not in self.entry_finder:\n",
    "                heappop(self.entries)\n",
    "                \n",
    "            return self.entries[0][1]\n",
    "        \n",
    "        def remove(self, x):\n",
    "            self.entry_finder.remove(x)\n",
    "    \n",
    "    def __init__(self, routerFactory, scorer, alpha, c, d, randomState, maxMemories=None):\n",
    "        self.routerFactory = routerFactory\n",
    "        self.f = scorer\n",
    "        self.alpha = alpha\n",
    "        self.c = c\n",
    "        self.d = d\n",
    "        self.leafbykey = {}\n",
    "        self.root = CMT.Node(None)\n",
    "        self.randomState = randomState        \n",
    "        self.allkeys = []\n",
    "        self.allkeysindex = {}\n",
    "        self.maxMemories = maxMemories \n",
    "        self.keyslru = CMT.LRU()\n",
    "        self.rerouting = False\n",
    "        self.splitting = False\n",
    "\n",
    "    def nodeForeach(self, f, node=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "            \n",
    "        f(node)\n",
    "        if node.left:\n",
    "            self.nodeForeach(f, node.left)\n",
    "        if node.right:\n",
    "            self.nodeForeach(f, node.right)\n",
    "        \n",
    "    def __path(self, x, v):          \n",
    "        nodes = []\n",
    "        while not v.isLeaf:\n",
    "            a = v.right if v.g.predict(x) > 0 else v.left\n",
    "            nodes.append(v)\n",
    "            v = a\n",
    "            \n",
    "        return CMT.Path(nodes, v)\n",
    "        \n",
    "    def query(self, x, k, epsilon):\n",
    "        path = self.__path(x, self.root)\n",
    "        q = self.randomState.uniform(0, 1)\n",
    "        if q >= epsilon:\n",
    "            return (None, path.leaf.topk(x, k, self.f))\n",
    "        else:\n",
    "            i = self.randomState.randint(0, len(path.nodes))\n",
    "            if i < len(path.nodes):\n",
    "                a = self.randomState.choice( (path.nodes[i].left, path.nodes[i].right) )\n",
    "                l = self.__path(x, a).leaf\n",
    "                return ((path.nodes[i], a, 1/2), l.topk(x, k, self.f))\n",
    "            else:\n",
    "                return ((path.leaf, None, None), path.leaf.randk(k, self.randomState))\n",
    "            \n",
    "    def update(self, u, x, z, r):\n",
    "        if u is None:\n",
    "            pass\n",
    "        else:\n",
    "            (v, a, p) = u\n",
    "            if v.isLeaf:\n",
    "                self.f.update(x, z, r)\n",
    "            else:\n",
    "                from math import log\n",
    "\n",
    "                rhat = (r/p) * (1 if a == v.right else -1)\n",
    "                y = (1 - self.alpha) * rhat + self.alpha * (log(1e-2 + v.left.n) - log(1e-2 + v.right.n)) \n",
    "                signy = 1 if y > 0 else -1\n",
    "                absy = signy * y\n",
    "                v.g.update(x, signy, absy)\n",
    "                \n",
    "            for _ in range(self.d):\n",
    "                self.__reroute()\n",
    "                \n",
    "    def delete(self, x):\n",
    "        if x not in self.allkeysindex:\n",
    "            # deleting something not in the memory ...\n",
    "            assert False\n",
    "                    \n",
    "        ind = self.allkeysindex.pop(x)\n",
    "        lastx = self.allkeys.pop()\n",
    "        if ind < len(self.allkeys):\n",
    "            self.allkeys[ind] = lastx\n",
    "            self.allkeysindex[lastx] = ind\n",
    "                \n",
    "        if not self.rerouting:\n",
    "            self.keyslru.remove(x)\n",
    "                \n",
    "        v = self.leafbykey.pop(x)\n",
    "        \n",
    "        while v is not None:\n",
    "            v.n -= 1\n",
    "            if v.isLeaf:\n",
    "                omega = v.memories.pop(x)\n",
    "            else:\n",
    "                if v.n == 0:\n",
    "                    other = v.parent.left if v is v.parent.right else v.parent.right\n",
    "                    if other.isLeaf:\n",
    "                        for xprime in other.memories.keys():\n",
    "                            self.leafbykey[xprime] = v.parent\n",
    "\n",
    "                    v.parent.replaceNode(other)\n",
    "                    v = v.parent\n",
    "                    \n",
    "            assert v.n >= 0\n",
    "            v = v.parent\n",
    "            \n",
    "    def __insertLeaf(self, x, omega, v):\n",
    "        from math import log\n",
    "        \n",
    "        assert v.isLeaf\n",
    "\n",
    "        if x not in self.allkeysindex:          \n",
    "            self.allkeysindex[x] = len(self.allkeys)\n",
    "            self.allkeys.append(x)\n",
    "        \n",
    "        if not self.rerouting and not self.splitting:\n",
    "            self.keyslru.add(x)\n",
    "                        \n",
    "        if self.splitting or v.n < self.c:\n",
    "            assert x not in self.leafbykey\n",
    "            self.leafbykey[x] = v\n",
    "            assert x not in v.memories\n",
    "            v.memories[x] = omega\n",
    "            v.n += 1\n",
    "            assert v.n == len(v.memories)\n",
    "        else:\n",
    "            self.splitting = True\n",
    "            mem = v.makeInternal(g=self.routerFactory())\n",
    "            \n",
    "            while mem:\n",
    "                xprime, omegaprime = mem.popitem()\n",
    "                del self.leafbykey[xprime]\n",
    "                self.insert(xprime, omegaprime, v)\n",
    "                \n",
    "            self.insert(x, omega, v)\n",
    "            self.splitting = False\n",
    "                     \n",
    "    def insert(self, x, omega, v=None):\n",
    "        from math import log\n",
    "        \n",
    "        if x in self.leafbykey:\n",
    "            # duplicate memory ... need to merge values ...\n",
    "            assert False\n",
    "            \n",
    "        if v is None:\n",
    "            v = self.root\n",
    "        \n",
    "        while not v.isLeaf:\n",
    "            B = log(1e-2 + v.left.n) - log(1e-2 + v.right.n) \n",
    "            y = (1 - self.alpha) * v.g.predict(x) + self.alpha * B\n",
    "            signy = 1 if y > 0 else -1\n",
    "            v.g.update(x, signy, 1)\n",
    "            v.n += 1\n",
    "            v = v.right if v.g.predict(x) > 0 else v.left\n",
    "            \n",
    "        self.__insertLeaf(x, omega, v)\n",
    "        \n",
    "        if not self.rerouting and not self.splitting:\n",
    "            if self.maxMemories is not None and len(self.keyslru) > self.maxMemories:\n",
    "                oldest = self.keyslru.peek()\n",
    "                self.delete(oldest)\n",
    "\n",
    "            for _ in range(self.d):\n",
    "                self.__reroute()\n",
    "                            \n",
    "    def __reroute(self):\n",
    "        x = self.randomState.choice(self.allkeys)\n",
    "        omega = self.leafbykey[x].memories[x]\n",
    "        self.rerouting = True\n",
    "        self.delete(x)\n",
    "        self.insert(x, omega)\n",
    "        self.rerouting = False\n",
    "        \n",
    "        for k in self.leafbykey.keys():\n",
    "            assert k in self.leafbykey[k].memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     1,
     7,
     17,
     36,
     44,
     48,
     57,
     66,
     102,
     128
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structureValid test pass\n",
      "selfconsistent test pass\n",
      "maxmemories test pass\n"
     ]
    }
   ],
   "source": [
    "class CMTTests:\n",
    "    class LinearModel:\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            from sklearn import linear_model\n",
    "            \n",
    "            self.model = linear_model.SGDRegressor(*args, **kwargs)\n",
    "            \n",
    "        def predict(self, x):\n",
    "            from sklearn.exceptions import NotFittedError \n",
    "            try:\n",
    "                return self.model.predict(X=[x])[0]\n",
    "            except NotFittedError:\n",
    "                return 0\n",
    "        \n",
    "        def update(self, x, y, w):\n",
    "            self.model.partial_fit(X=[x], y=[y], sample_weight=[w])\n",
    "            \n",
    "    class NormalizedLinearProduct:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        \n",
    "        def predict(self, x, z):\n",
    "            import numpy as np\n",
    "            from math import sqrt\n",
    "            \n",
    "            (xprime, omegaprime) = z\n",
    "            \n",
    "            xa = np.array(x)\n",
    "            xprimea = np.array(xprime)\n",
    "                        \n",
    "            return np.inner(xa, xprimea) / sqrt(np.inner(xa, xa) * np.inner(xprimea, xprimea))\n",
    "        \n",
    "        def update(self, x, y, w):\n",
    "            pass\n",
    " \n",
    "    @staticmethod\n",
    "    def displaynode(node, indent):\n",
    "        if node is not None:\n",
    "            from pprint import pformat\n",
    "            print(indent, pformat((node, node.__dict__)))\n",
    "            CMTTests.displaynode(node.left, indent + \"*\")\n",
    "            CMTTests.displaynode(node.right, indent + \"*\")\n",
    "\n",
    "    @staticmethod\n",
    "    def displaytree(cmt):\n",
    "        CMTTests.displaynode(cmt.root, indent=\"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def structureValid():\n",
    "        import random\n",
    "        \n",
    "        routerFactory = lambda: CMTTests.LinearModel()\n",
    "        scorer = CMTTests.NormalizedLinearProduct()\n",
    "        randomState = random.Random()\n",
    "        randomState.seed(2112)\n",
    "        cmt = CMT(routerFactory=routerFactory, scorer=scorer, alpha=0.5, c=10, d=0, randomState=randomState)\n",
    "\n",
    "        def checkNodeInvariants(node):\n",
    "            assert node.parent is None or node.parent.left is node or node.parent.right is node\n",
    "            assert node.left is None or node.n == node.left.n + node.right.n\n",
    "            assert node.left is None or node.left.parent is node\n",
    "            assert node.right is None or node.right.parent is node\n",
    "            assert node.left is not None or node.n == len(node.memories)\n",
    "    \n",
    "        stuff = {}\n",
    "        \n",
    "        for _ in range(200):\n",
    "            try:\n",
    "                if stuff and randomState.uniform(0, 1) < 0.1:\n",
    "                    # delete\n",
    "                    x, omega = stuff.popitem()\n",
    "                    cmt.delete(x)\n",
    "                elif stuff and randomState.uniform(0, 1) < 0.1:\n",
    "                    # query/update\n",
    "                    somex = randomState.choice(list(stuff.keys()))\n",
    "                    u, z = cmt.query(x, 1, 0.1)\n",
    "                    cmt.update(u, x, z, randomState.uniform(0, 1))\n",
    "                else:\n",
    "                    # insert\n",
    "                    x = tuple([ randomState.uniform(0, 1) for _ in range(3)])\n",
    "                    omega = randomState.uniform(0, 1)\n",
    "                    cmt.insert(x, omega)\n",
    "                    stuff[x] = omega\n",
    "\n",
    "                assert cmt.root.n == len(stuff)\n",
    "                assert cmt.root.n == len(cmt.leafbykey)\n",
    "                assert cmt.root.n == len(cmt.allkeys)\n",
    "                assert cmt.root.n == len(cmt.allkeysindex)\n",
    "                for x in stuff.keys():\n",
    "                    assert x in cmt.leafbykey[x].memories\n",
    "                    assert x in cmt.allkeysindex\n",
    "                    assert cmt.allkeys[cmt.allkeysindex[x]] is x\n",
    "                cmt.nodeForeach(checkNodeInvariants)\n",
    "            except:\n",
    "                print(\"--------------\")\n",
    "                CMTTests.displaytree(cmt)\n",
    "                print(\"--------------\")\n",
    "                raise\n",
    "                \n",
    "        print('structureValid test pass')           \n",
    "                       \n",
    "    @staticmethod\n",
    "    def selfconsistent():\n",
    "        import random\n",
    "        \n",
    "        routerFactory = lambda: CMTTests.LinearModel()\n",
    "        scorer = CMTTests.NormalizedLinearProduct()\n",
    "        randomState = random.Random()\n",
    "        randomState.seed(45)\n",
    "        cmt = CMT(routerFactory=routerFactory, scorer=scorer, alpha=0.5, c=10, d=0, randomState=randomState)\n",
    "        \n",
    "        for _ in range(200):\n",
    "            try:\n",
    "                x = tuple([ randomState.uniform(0, 1) for _ in range(3)])\n",
    "                omega = randomState.uniform(0, 1)\n",
    "\n",
    "                cmt.insert(x, omega)\n",
    "                u, [ (xprime, omegaprime) ] = cmt.query(x, k=1, epsilon=0)\n",
    "                assert omega == omegaprime, '({}, [({}, {})]) = cmt.query({}) != {}'.format(u, xprime, omegaprime, x, omega)\n",
    "            except:\n",
    "                print(\"--------------\")\n",
    "                CMTTests.displaytree(cmt)\n",
    "                print(\"--------------\")\n",
    "                raise\n",
    "                \n",
    "        print('selfconsistent test pass')\n",
    "        \n",
    "    @staticmethod\n",
    "    def maxmemories():\n",
    "        import random\n",
    "        \n",
    "        routerFactory = lambda: CMTTests.LinearModel()\n",
    "        scorer = CMTTests.NormalizedLinearProduct()\n",
    "        randomState = random.Random()\n",
    "        randomState.seed(45)\n",
    "        maxM = 100\n",
    "        cmt = CMT(routerFactory=routerFactory, scorer=scorer, alpha=0.5, c=10, d=0, randomState=randomState, maxMemories=maxM)\n",
    "        \n",
    "        for _ in range(200):\n",
    "            try:\n",
    "                x = tuple([ randomState.uniform(0, 1) for _ in range(3)])\n",
    "                omega = randomState.uniform(0, 1)\n",
    "\n",
    "                cmt.insert(x, omega)\n",
    "                assert len(cmt.leafbykey) <= maxM\n",
    "            except:\n",
    "                print(\"--------------\")\n",
    "                CMTTests.displaytree(cmt)\n",
    "                print(\"--------------\")\n",
    "                raise\n",
    "                \n",
    "        print('maxmemories test pass')\n",
    "       \n",
    "    @staticmethod\n",
    "    def all():\n",
    "        CMTTests.structureValid()\n",
    "        CMTTests.selfconsistent()\n",
    "        CMTTests.maxmemories()\n",
    "\n",
    "CMTTests().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Observed Covertype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     15,
     57
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 283301, 0: 211840, 2: 35754, 6: 20510, 5: 17367, 4: 9493, 3: 2747})\n",
      "*** lr = 0.1 ***\n",
      "n       \temp loss\tsince last\tpred      \n",
      "64      \t0.734   \t0.734     \t1         \n",
      "96      \t0.760   \t0.812     \t1         \n",
      "160     \t0.769   \t0.781     \t5         \n",
      "288     \t0.691   \t0.594     \t5         \n",
      "544     \t0.656   \t0.617     \t1         \n",
      "1056    \t0.591   \t0.521     \t1         \n",
      "2080    \t0.506   \t0.418     \t0         \n",
      "4128    \t0.436   \t0.365     \t2         \n",
      "8224    \t0.379   \t0.323     \t0         \n",
      "16416   \t0.340   \t0.301     \t0         \n",
      "32800   \t0.317   \t0.294     \t0         \n",
      "65568   \t0.304   \t0.291     \t1         \n",
      "131104  \t0.295   \t0.285     \t0         \n",
      "262176  \t0.290   \t0.285     \t1         \n",
      "522880  \t0.287   \t0.284     \t0         \n",
      "Counter({1: 30323, 0: 20779, 2: 4863, 6: 1508, 5: 417, 3: 198, 4: 13})\n",
      "test accuracy: [0.71638612 0.71909261 0.72182493]\n",
      "19.09426959976554\n"
     ]
    }
   ],
   "source": [
    "class FOC:\n",
    "    class EasyAcc:\n",
    "        def __init__(self):\n",
    "            self.n = 0\n",
    "            self.sum = 0\n",
    "            \n",
    "        def __iadd__(self, other):\n",
    "            self.n += 1\n",
    "            self.sum += other\n",
    "            return self\n",
    "            \n",
    "        def mean(self):\n",
    "            return self.sum / max(self.n, 1)\n",
    " \n",
    "    import torch\n",
    "    class LogisticRegressor(torch.nn.Module):        \n",
    "        def __init__(self, input_dim, output_dim, eta0=0.1):\n",
    "            import torch\n",
    "            \n",
    "            super(FOC.LogisticRegressor, self).__init__()\n",
    "            self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "            self.loss = torch.nn.CrossEntropyLoss()\n",
    "            self.optimizer = torch.optim.Adam(self.linear.parameters(), lr=eta0)\n",
    "            self.eta0 = eta0\n",
    "            self.n = 0\n",
    "            \n",
    "        def forward(self, x):\n",
    "            import numpy as np\n",
    "            import torch\n",
    "\n",
    "            return self.linear(torch.autograd.Variable(torch.from_numpy(x)))\n",
    "        \n",
    "        def predict(self, X):\n",
    "            import torch\n",
    "            \n",
    "            return torch.argmax(self.forward(X), dim=1).numpy()\n",
    "        \n",
    "        def set_lr(self):\n",
    "            from math import sqrt\n",
    "            lr = self.eta0 / sqrt(self.n)\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "            \n",
    "        def partial_fit(self, X, y, sample_weight=None, **kwargs):\n",
    "            import torch\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            yhat = self.forward(X)\n",
    "            if sample_weight is None:\n",
    "                loss = self.loss(yhat, torch.from_numpy(y))\n",
    "            else:\n",
    "                loss = sample_weight * self.loss(yhat, torch.from_numpy(y))\n",
    "            loss.backward()\n",
    "            self.n += X.shape[0]\n",
    "            self.set_lr()\n",
    "            self.optimizer.step() \n",
    "        \n",
    "    def doit():\n",
    "        from collections import Counter\n",
    "        from sklearn.datasets import fetch_covtype\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from math import ceil\n",
    "        import numpy as np\n",
    "\n",
    "        cov = fetch_covtype()\n",
    "        cov.data = PCA(whiten=True).fit_transform(cov.data).astype(np.float32)\n",
    "        classes = np.unique(cov.target - 1)\n",
    "        print(Counter(cov.target - 1))\n",
    "        ndata = len(cov.target)\n",
    "        order = np.random.RandomState(seed=42).permutation(ndata)\n",
    "        ntrain = ceil(0.9 * ndata)\n",
    "        Object = lambda **kwargs: type(\"Object\", (), kwargs)()\n",
    "        train = Object(data = cov.data[order[:ntrain]], target = cov.target[order[:ntrain]] - 1)\n",
    "        test = Object(data = cov.data[order[ntrain:]], target = cov.target[order[ntrain:]] - 1)\n",
    "  \n",
    "        blocksize = 32\n",
    "        for lr in [0.1]:\n",
    "            print(\"*** lr = {} ***\".format(lr), flush=True)\n",
    "            print('{:8.8s}\\t{:8.8s}\\t{:10.10s}\\t{:10.10s}'.format(\n",
    "                'n', 'emp loss', 'since last', 'pred')\n",
    "            )\n",
    "            \n",
    "            input_dim = train.data[0].shape[0]\n",
    "            cls = FOC.LogisticRegressor(input_dim, output_dim=len(classes), eta0=lr)\n",
    "#             cls = SGDClassifier(loss='log', shuffle=False, learning_rate='invscaling', eta0=lr)\n",
    "            loss = FOC.EasyAcc()\n",
    "            sincelast = FOC.EasyAcc()\n",
    "\n",
    "            for pno in range(1):\n",
    "                order = np.random.RandomState(seed=42+pno).permutation(len(train.data))\n",
    "                for n, ind in enumerate(zip(*(iter(order),)*blocksize)):\n",
    "                    v = np.array([ t\n",
    "                                   for z in ind for t in ( train.data[z], ) ],\n",
    "                                 dtype='float32')\n",
    "                    actual = np.array([ train.target[z] for z in ind ], dtype='int')\n",
    "                    if n == 0:\n",
    "                        for a in actual:\n",
    "                            loss += 0 if a == 0 else 1\n",
    "                            sincelast += 0 if a == 0 else 1\n",
    "                    if n > 0:\n",
    "                        pred = cls.predict(v)\n",
    "                        for p, a in zip(pred, actual):\n",
    "                            loss += 0 if p == a else 1\n",
    "                            sincelast += 0 if p == a else 1  \n",
    "\n",
    "                        if (n & (n - 1) == 0): # and n & 0xAAAAAAAA == 0):\n",
    "                            print('{:<8d}\\t{:<8.3f}\\t{:<10.3f}\\t{:<10d}'.format(\n",
    "                                        loss.n, loss.mean(), sincelast.mean(), pred[-1]),\n",
    "                                  flush=True)\n",
    "\n",
    "                            sincelast = FOC.EasyAcc()\n",
    "\n",
    "                    cls.partial_fit(v, actual, classes=classes)\n",
    "\n",
    "                print('{:<8d}\\t{:<8.3f}\\t{:<10.3f}\\t{:<10d}'.format(\n",
    "                             loss.n, loss.mean(), sincelast.mean(), pred[-1]),\n",
    "                       flush=True)                \n",
    "                sincelast = FOC.EasyAcc()\n",
    "\n",
    "                preds = cls.predict(test.data.astype('float32'))\n",
    "                print(Counter(preds))\n",
    "                ascores = []\n",
    "                for b in range(16):\n",
    "                    bootie = np.random.RandomState(90210+b).choice(len(test.target), replace=True, size=len(test.target))\n",
    "                    ascores.append(accuracy_score(y_true=test.target[bootie], y_pred=preds[bootie]))\n",
    "\n",
    "                print(\"test accuracy: {}\".format(np.quantile(ascores, [0.05, 0.5, 0.95])))\n",
    "\n",
    "def flass():\n",
    "    import timeit\n",
    "    print(timeit.timeit(FOC.doit, number=1))\n",
    "    \n",
    "flass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Memory Tree (Linear Routers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     1,
     16,
     26,
     32,
     43,
     57,
     73,
     126
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 283301, 0: 211840, 2: 35754, 6: 20510, 5: 17367, 4: 9493, 3: 2747})\n",
      "n       \temp loss\tsince last\tlast pred \n",
      "1       \t1.000   \t1.000     \t0         \n",
      "2       \t0.500   \t0.000     \t0         \n",
      "3       \t0.333   \t0.000     \t0         \n",
      "5       \t0.400   \t0.500     \t0         \n",
      "9       \t0.333   \t0.250     \t1         \n",
      "17      \t0.529   \t0.750     \t1         \n",
      "33      \t0.455   \t0.375     \t0         \n",
      "65      \t0.446   \t0.438     \t1         \n",
      "129     \t0.442   \t0.438     \t1         \n",
      "257     \t0.436   \t0.430     \t1         \n",
      "513     \t0.390   \t0.344     \t1         \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-94f4ab0230af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0mflass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-94f4ab0230af>\u001b[0m in \u001b[0;36mflass\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mflass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0mflass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/homer/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[1;32m    231\u001b[0m            number=default_number, globals=None):\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m def repeat(stmt=\"pass\", setup=\"pass\", timer=default_timer,\n",
      "\u001b[0;32m~/miniconda3/envs/homer/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/homer/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-94f4ab0230af>\u001b[0m in \u001b[0;36mdoit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;31m#                     cmt.update(u, x, z, 1 if pred == actual else -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                     \u001b[0mcmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             print('{:<8d}\\t{:<8.3f}\\t{:<10.3f}\\t{:<10d}'.format(\n",
      "\u001b[0;32m<ipython-input-1-169e70ed6c0e>\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, x, omega, v)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-169e70ed6c0e>\u001b[0m in \u001b[0;36m__reroute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerouting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerouting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-169e70ed6c0e>\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, x, omega, v)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0msigny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-94f4ab0230af>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, x, y, w)\u001b[0m\n\u001b[1;32m     70\u001b[0m                                    \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                    classes=(0, 1))\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mEuclideanDistance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-94f4ab0230af>\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/homer/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/homer/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class FOC:\n",
    "    class EasyAcc:\n",
    "        def __init__(self):\n",
    "            self.n = 0\n",
    "            self.sum = 0\n",
    "            \n",
    "        def __iadd__(self, other):\n",
    "            self.n += 1\n",
    "            self.sum += other\n",
    "            return self\n",
    "            \n",
    "        def mean(self):\n",
    "            return self.sum / max(self.n, 1)\n",
    " \n",
    "    import torch\n",
    "    class LogisticRegressor(torch.nn.Module):        \n",
    "        def __init__(self, input_dim, output_dim, eta0):\n",
    "            import torch\n",
    "            \n",
    "            super(FOC.LogisticRegressor, self).__init__()\n",
    "            self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "            self.loss = torch.nn.CrossEntropyLoss()\n",
    "            self.optimizer = torch.optim.Adam(self.linear.parameters(), lr=eta0)\n",
    "            self.eta0 = eta0\n",
    "            self.n = 0\n",
    "            \n",
    "        def forward(self, x):\n",
    "            import numpy as np\n",
    "            import torch\n",
    "\n",
    "            return self.linear(torch.autograd.Variable(torch.from_numpy(x)))\n",
    "        \n",
    "        def predict(self, X):\n",
    "            import torch\n",
    "            \n",
    "            return torch.argmax(self.forward(X), dim=1).numpy()\n",
    "        \n",
    "        def set_lr(self):\n",
    "            from math import sqrt\n",
    "            lr = self.eta0 / sqrt(self.n)\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "\n",
    "        def partial_fit(self, X, y, sample_weight=None, **kwargs):\n",
    "            import torch\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            yhat = self.forward(X)\n",
    "            if sample_weight is None:\n",
    "                loss = self.loss(yhat, torch.from_numpy(y))\n",
    "            else:\n",
    "                loss = torch.from_numpy(sample_weight) * self.loss(yhat, torch.from_numpy(y))\n",
    "            loss.backward()\n",
    "            self.n += X.shape[0]\n",
    "            self.set_lr()\n",
    "            self.optimizer.step() \n",
    "\n",
    "    class LogisticModel:\n",
    "        def __init__(self, *args, **kwargs):            \n",
    "            self.model = FOC.LogisticRegressor(*args, **kwargs)\n",
    "            \n",
    "        def predict(self, x):\n",
    "            import numpy as np\n",
    "            return -1 + 2 * self.model.predict(X=np.array([x], dtype='float32'))[0]\n",
    "        \n",
    "        def update(self, x, y, w):\n",
    "            import numpy as np\n",
    "            \n",
    "            self.model.partial_fit(X=np.array([x], dtype='float32'), \n",
    "                                   y=(1 + np.array([y], dtype='int')) // 2, \n",
    "                                   sample_weight=np.array([w], dtype='float32'),\n",
    "                                   classes=(0, 1))\n",
    "\n",
    "    class EuclideanDistance:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        \n",
    "        def predict(self, x, z):\n",
    "            import numpy as np\n",
    "            from math import sqrt\n",
    "            \n",
    "            (xprime, omegaprime) = z\n",
    "            \n",
    "            return -np.linalg.norm(np.array(x) - np.array(xprime))\n",
    "        \n",
    "        def update(self, x, y, w):\n",
    "            pass\n",
    "            \n",
    "    def doit():\n",
    "        from collections import Counter\n",
    "        from sklearn.datasets import fetch_covtype\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from math import ceil\n",
    "        import numpy as np\n",
    "        import random\n",
    "        import torch\n",
    "\n",
    "        cov = fetch_covtype()\n",
    "        cov.data = PCA(whiten=True).fit_transform(cov.data)\n",
    "        classes = np.unique(cov.target - 1)\n",
    "        print(Counter(cov.target - 1))\n",
    "        ndata = len(cov.target)\n",
    "        order = np.random.RandomState(seed=42).permutation(ndata)\n",
    "        ntrain = ceil(0.9 * ndata)\n",
    "        Object = lambda **kwargs: type(\"Object\", (), kwargs)()\n",
    "        train = Object(data = cov.data[order[:ntrain]], target = cov.target[order[:ntrain]] - 1)\n",
    "        test = Object(data = cov.data[order[ntrain:]], target = cov.target[order[ntrain:]] - 1)\n",
    "        \n",
    "        input_dim = train.data[0].shape[0]\n",
    "        routerFactory = lambda: FOC.LogisticModel(eta0=0.1, input_dim=input_dim, output_dim=2)\n",
    "        scorer = FOC.EuclideanDistance()\n",
    "        randomState = random.Random()\n",
    "        randomState.seed(45)\n",
    "        torch.manual_seed(2112)\n",
    "        cmt = CMT(routerFactory=routerFactory, scorer=scorer, alpha=0.25, c=10, d=1, randomState=randomState, \n",
    "                  maxMemories=None)\n",
    "  \n",
    "        print('{:8.8s}\\t{:8.8s}\\t{:10.10s}\\t{:10.10s}'.format(\n",
    "            'n', 'emp loss', 'since last', 'last pred')\n",
    "        )\n",
    "\n",
    "        loss = FOC.EasyAcc()\n",
    "        sincelast = FOC.EasyAcc()\n",
    "\n",
    "        for pno in range(1):\n",
    "            order = np.random.RandomState(seed=42+pno).permutation(len(train.data))\n",
    "            for n, ind in enumerate(order):\n",
    "                t = train.data[ind]\n",
    "                x = tuple(t)\n",
    "                actual = train.target[ind]\n",
    "                \n",
    "                if n == 0:\n",
    "                    pred = 0\n",
    "                else:\n",
    "                    u, z = cmt.query(x, k=1, epsilon=0.0)\n",
    "                    pred = z[0][1] if len(z) else 0\n",
    "                    \n",
    "                loss += 0 if pred == actual else 1\n",
    "                sincelast += 0 if pred == actual else 1\n",
    "                \n",
    "                if (n & (n - 1) == 0): # and n & 0xAAAAAAAA == 0):\n",
    "                    print('{:<8d}\\t{:<8.3f}\\t{:<10.3f}\\t{:<10d}'.format(\n",
    "                                loss.n, loss.mean(), sincelast.mean(), pred),\n",
    "                          flush=True)\n",
    "\n",
    "                    sincelast = FOC.EasyAcc()\n",
    "                    \n",
    "                if n > 0:\n",
    "#                     cmt.update(u, x, z, 1 if pred == actual else -1)\n",
    "                    cmt.insert(x, actual)\n",
    "\n",
    "            print('{:<8d}\\t{:<8.3f}\\t{:<10.3f}\\t{:<10d}'.format(\n",
    "                         loss.n, loss.mean(), sincelast.mean(), pred),\n",
    "                   flush=True)                \n",
    "            sincelast = FOC.EasyAcc()\n",
    "            \n",
    "#             preds = cls.predict(np.array([np.outer(d, np.append(d, [1])).ravel() for d in test.data]))\n",
    "#             print(Counter(preds))\n",
    "#             ascores = []\n",
    "#             for b in range(16):\n",
    "#                 bootie = np.random.RandomState(90210+b).choice(len(test.target), replace=True, size=len(test.target))\n",
    "#                 ascores.append(accuracy_score(y_true=test.target[bootie], y_pred=preds[bootie]))\n",
    "\n",
    "#             print(\"test accuracy: {}\".format(np.quantile(ascores, [0.05, 0.5, 0.95])))\n",
    "\n",
    "            pass\n",
    "                        \n",
    "def flass():\n",
    "    import timeit\n",
    "    print(timeit.timeit(FOC.doit, number=1))\n",
    "    \n",
    "flass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Memorized Q pseudocode -- Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basic idea:\n",
    "* Estimate value of $a$ in context $x$ by stored value associated with first memory retrieved from CMT queried with $(x, a)$.\n",
    "* Play $\\epsilon$-greedy with greedy action being the maximum estimated value. \n",
    "* Play action $a$ in context $x$ and observe reward $r$.\n",
    "* Reward memory system just like a parametric direct method, i.e., using regression loss such as squared loss.\n",
    "   * Update the memory $((x', a'), r')$ retrieved by query $(x, a)$ using reward $-(r - r')^2$.\n",
    "* Insert key $(x, a)$ with value $r$.\n",
    "* Conjecture: compatible with self-consistency assuming no reward variance.\n",
    "   * Update reward is maximized by retrieving a memory with $r = r'$.\n",
    "   * Exact match response does this.\n",
    "   * Censorship issue: only argmax key is updated, does this matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Memorized Q pseudocode -- Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basic idea:\n",
    "* Estimate value of $a$ in context $x$ by stored value associated with first memory retrieved from CMT queried with $(x, a)$.\n",
    "* Play $\\epsilon$-greedy with greedy action being the maximum estimated value. \n",
    "* Play action $a$ in context $x$ and observe reward $r$.\n",
    "* For each action $a'$, update the memory retrieved with query $(x, a')$ using the observed reward as feedback reward.\n",
    "* Insert key $(x, a)$ with value $r$.\n",
    "* Conjecture: compatible with self-consistency assuming no reward variance.\n",
    "   * Update reward is maximized by identifying the correct argmax.\n",
    "   * Exact match responses to all queries identifies the correct argmax.\n",
    "   * No censorship issues: all retrieved keys are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Memorized Q pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basic idea:\n",
    "* Estimate value of $a$ in context $x$ by stored value associated with first memory retrieved from CMT queried with $(x, a)$.\n",
    "* Play $\\epsilon$-greedy with greedy action being the maximum estimated value. \n",
    "* After playing action $a$ in context $x$ and observing reward $r$:\n",
    "   * Update the memory retrieved with query $(x, a)$ using feedback reward of $r$.\n",
    "   * Store memory with key $(x, a)$ and value $r$.\n",
    "* Conjecture: not compatible with self-consistency because of update frequency issues.\n",
    "   * By returning a non-exact match with high stored reward, a key can capture more updates.\n",
    "   * Counter argument: the memory system as a whole receives largest possible reward if argmax is correct, which exact match ensures.\n",
    "   * Counter Counter argument: but reward is associated to particular keys with different frequency, does that matter?\n",
    "   * **Confused**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MemorizedQ():\n",
    "    mem = CMT()\n",
    "    env = Environment()  # distribution over (x, r) pairs.\n",
    "\n",
    "    Actions = set(...)   # fixed set of actions for now\n",
    "    epsilon = ...        # epsilon-greedy exploration\n",
    "\n",
    "    while True:\n",
    "        x = env.Observe()\n",
    "        querySet = { a: (u, ((xprime, aprime), rprime))\n",
    "                     for a in Actions\n",
    "                     for (u, z) in [ CMT.Query(key=(x, a)) ]\n",
    "                     if len(z) > 0\n",
    "                     for ((xprime, aprime), rprime) in [ z[0] ]\n",
    "                   }\n",
    "        if len(querySet) > 0:\n",
    "            greedy, _ = max(querySet.iteritems(), lambda kv: kv[1][1][1]) # action with largest first retrieved reward\n",
    "        else:\n",
    "            greedy = next(iter(Actions))                                  # if memory is completely empty, play action 0\n",
    "\n",
    "        pa = (1 - epsilon) * IndicatorDistribution(greedy) + epsilon * UniformDistribution(Actions)\n",
    "        a = pa.sample()\n",
    "        r = env.ObserveReward(a)\n",
    "\n",
    "        if a in querySet:\n",
    "            # question: what's the feedback reward?\n",
    "            # question: do we only do this when we take the greedy action?\n",
    "\n",
    "            u, (xprime, aprime), rprime = querySet[a]\n",
    "            CMT.Update(key=(x, a), retrieved=((xprime, aprime), rprime), feedbackreward=None, u=u)\n",
    "\n",
    "        CMT.Insert(key=(x, a), value=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Is this compatible with self-consistency?\n",
    "\n",
    "I'm not sure.  Suppose there is no reward variance, so we just dealing with the partial feedback issue.\n",
    "* Any memory retrieved when querying on $(x, a)$ will be updated with feedback reward $r$.\n",
    "* Conditional on calling `Update()`, feedback reward is constant.\n",
    "* Except that some retrieved memories will \"win the argmax\" and some will lose, changing frequency of `Update()`.\n",
    "* Consider the memory retrieved by `Query(key=(x, a))`.\n",
    "   * Possible inserted $((x, a), r)$ pair will lose the argmax after additional inserts.\n",
    "   * This could be appropriate as another action $a'$ might be better in a neighborhood of $x$ but hadn't been observed yet.\n",
    "   * However retrieving $((x'', a''), r'')$ with $r'' > r$ would win the argmax and receive reward $r$.\n",
    "   \n",
    "**Idea**: this could be self-consistent if we update all the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
