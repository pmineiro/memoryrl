{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: memorize the $Q$ function qua [Model-Free Episodic Control](https://arxiv.org/abs/1606.04460)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMT API\n",
    "\n",
    "From the paper we have:\n",
    "\n",
    "1. $(u, z) \\leftarrow \\text{Query}(x)$ where $z = \\{ (x_n, \\omega_n) \\}$ is an ordered set of retrieved key-value pairs.\n",
    "1. $\\text{Update}(x, (x_n, \\omega_n), r, u)$ provides feedback reward $r$ for retrieval of $(x_n, \\omega_n)$ for query $x$.\n",
    "   1. Must be compatible with self-consistency or supervised and unsupervised updates conflict.\n",
    "1. $\\text{Insert}(x, \\omega)$ creates a new memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Memorized Q pseudocode -- Attempt 2\n",
    "\n",
    "Basic idea:\n",
    "* Estimate value of $a$ in context $x$ by stored value associated with first memory retrieved from CMT queried with $(x, a)$.\n",
    "* Play $\\epsilon$-greedy with greedy action being the maximum estimated value. \n",
    "* Play action $a$ in context $x$ and observe reward $r$.\n",
    "* For each action $a'$, update the memory retrieved with query $(x, a')$ $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorized Q pseudocode\n",
    "\n",
    "Basic idea:\n",
    "* Estimate value of $a$ in context $x$ by stored value associated with first memory retrieved from CMT queried with $(x, a)$.\n",
    "* Play $\\epsilon$-greedy with greedy action being the maximum estimated value. \n",
    "* After playing action $a$ in context $x$ and observing reward $r$:\n",
    "   * Update the memory retrieved with query $(x, a)$ using feedback reward of $r$.\n",
    "   * Store memory with key $(x, a)$ and value $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MemorizedQ():\n",
    "    mem = CMT()\n",
    "    env = Environment()  # distribution over (x, r) pairs.\n",
    "\n",
    "    Actions = set(...)   # fixed set of actions for now\n",
    "    epsilon = ...        # epsilon-greedy exploration\n",
    "\n",
    "    while True:\n",
    "        x = env.Observe()\n",
    "        querySet = { a: (u, ((xprime, aprime), rprime))\n",
    "                     for a in Actions\n",
    "                     for (u, z) in [ CMT.Query(key=(x, a)) ]\n",
    "                     if len(z) > 0\n",
    "                     for ((xprime, aprime), rprime) in [ z[0] ]\n",
    "                   }\n",
    "        if len(querySet) > 0:\n",
    "            greedy, _ = max(querySet.iteritems(), lambda kv: kv[1][1][1]) # action with largest first retrieved reward\n",
    "        else:\n",
    "            greedy = next(iter(Actions))                                  # if memory is completely empty, play action 0\n",
    "\n",
    "        pa = (1 - epsilon) * IndicatorDistribution(greedy) + epsilon * UniformDistribution(Actions)\n",
    "        a = pa.sample()\n",
    "        r = env.ObserveReward(a)\n",
    "\n",
    "        if a in querySet:\n",
    "            # question: what's the feedback reward?\n",
    "            # question: do we only do this when we take the greedy action?\n",
    "\n",
    "            u, (xprime, aprime), rprime = querySet[a]\n",
    "            CMT.Update(key=(x, a), retrieved=((xprime, aprime), rprime), feedbackreward=None, u=u)\n",
    "\n",
    "        CMT.Insert(key=(x, a), value=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this compatible with self-consistency?\n",
    "\n",
    "I'm not sure.  Suppose there is no reward variance, so we just dealing with the partial feedback issue.\n",
    "* Any memory retrieved when querying on $(x, a)$ will be updated with feedback reward $r$.\n",
    "* Conditional on calling `Update()`, feedback reward is constant.\n",
    "* Except that some retrieved memories will \"win the argmax\" and some will lose, changing frequency of `Update()`.\n",
    "* Consider the memory retrieved by `Query(key=(x, a))`.\n",
    "   * Possible inserted $((x, a), r)$ pair will lose the argmax after additional inserts.\n",
    "   * This could be appropriate as another action $a'$ might be better in a neighborhood of $x$ but hadn't been observed yet.\n",
    "   * However retrieving $((x'', a''), r'')$ with $r'' > r$ would win the argmax and receive reward $r$.\n",
    "   \n",
    "**Idea**: this could be self-consistent if we update all the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
